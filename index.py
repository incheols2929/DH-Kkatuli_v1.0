from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse, HTMLResponse
from llama_cpp import Llama
import json
import asyncio
import logging
import os
import psutil
import re
from datetime import datetime
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict


# ë¡œê¹… ì„¤ì • ê°•í™”
def setup_logging():
    """í–¥ìƒëœ ë¡œê¹… ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    # í˜„ì¬ ë‚ ì§œë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
    log_filename = f"{log_dir}/kkatuli_{datetime.now().strftime('%Y%m%d')}.log"

    # ë¡œê¹… í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )

    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ëª¨ë“  ë¡œê·¸ë¥¼ íŒŒì¼ì— ì €ì¥)
    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)

    # ì½˜ì†” í•¸ë“¤ëŸ¬ (INFO ì´ìƒë§Œ ì½˜ì†”ì— ì¶œë ¥)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)

    # í•¸ë“¤ëŸ¬ê°€ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # ìƒˆ í•¸ë“¤ëŸ¬ ì¶”ê°€
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)

    return logging.getLogger(__name__)


# ë¡œê¹… ì„¤ì • ì´ˆê¸°í™”
logger = setup_logging()

# ëª¨ë¸ì„ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸í•˜ë˜ ì´ˆê¸°í™”ëŠ” ì§€ì—°
llm = None

# ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ì „ì—­ ë”•ì…”ë„ˆë¦¬ (ì„¸ì…˜ IDë³„ë¡œ ê´€ë¦¬)
conversation_history: Dict[str, List[Dict[str, str]]] = {}

# ì„±ëŠ¥ í†µê³„ë¥¼ ìœ„í•œ ê¸€ë¡œë²Œ ë³€ìˆ˜
performance_stats = {
    "total_requests": 0,
    "total_tokens_generated": 0,
    "total_processing_time": 0,
    "error_count": 0
}


def log_performance_stats():
    """ì„±ëŠ¥ í†µê³„ ë¡œê¹…"""
    logger.info("=" * 50)
    logger.info("ì„±ëŠ¥ í†µê³„ ìš”ì•½:")
    logger.info(f"ì´ ìš”ì²­ ìˆ˜: {performance_stats['total_requests']}")
    logger.info(f"ì´ ìƒì„±ëœ í† í° ìˆ˜: {performance_stats['total_tokens_generated']}")
    logger.info(f"ì´ ì²˜ë¦¬ ì‹œê°„: {performance_stats['total_processing_time']:.2f}ì´ˆ")
    logger.info(
        f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {performance_stats['total_processing_time'] / max(1, performance_stats['total_requests']):.2f}ì´ˆ/ìš”ì²­")
    logger.info(
        f"í‰ê·  í† í° ìƒì„± ì†ë„: {performance_stats['total_tokens_generated'] / max(1, performance_stats['total_processing_time']):.2f}í† í°/ì´ˆ")
    logger.info(f"ì˜¤ë¥˜ ë°œìƒ ìˆ˜: {performance_stats['error_count']}")
    logger.info("=" * 50)


def optimize_system():
    """ì‹œìŠ¤í…œ ìµœì í™” ì„¤ì •"""
    logger.info("ì‹œìŠ¤í…œ ìµœì í™” ì‹œì‘...")
    try:
        # CPU ì •ë³´ ë¡œê¹…
        cpu_count = psutil.cpu_count()
        memory_info = psutil.virtual_memory()
        logger.info(f"CPU ì½”ì–´ ìˆ˜: {cpu_count}")
        logger.info(f"ì „ì²´ ë©”ëª¨ë¦¬: {memory_info.total / (1024 ** 3):.1f}GB")
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {memory_info.available / (1024 ** 3):.1f}GB")

        # CPU ì¹œí™”ë„ ì„¤ì • (ëª¨ë“  ì½”ì–´ ì‚¬ìš©)
        process = psutil.Process()
        process.cpu_affinity(list(range(cpu_count)))
        logger.info(f"CPU ì¹œí™”ë„ ì„¤ì • ì™„ë£Œ: ëª¨ë“  {cpu_count}ê°œ ì½”ì–´ ì‚¬ìš©")

        # ìš°ì„ ìˆœìœ„ ë†’ìŒìœ¼ë¡œ ì„¤ì •
        process.nice(psutil.HIGH_PRIORITY_CLASS)
        logger.info("í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ë¥¼ ë†’ìŒìœ¼ë¡œ ì„¤ì • ì™„ë£Œ")

    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(traceback.format_exc())


def initialize_model():
    """ëª¨ë¸ì„ ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”"""
    global llm
    if llm is None:
        try:
            logger.info("=" * 50)
            logger.info("ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            start_time = datetime.now()

            model_path = "model/EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf"
            logger.info(f"ëª¨ë¸ íŒŒì¼: {model_path}")

            # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(model_path):
                logger.error(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
                raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

            # ëª¨ë¸ íŒŒì¼ í¬ê¸° í™•ì¸
            model_size = os.path.getsize(model_path) / (1024 ** 3)
            logger.info(f"ëª¨ë¸ íŒŒì¼ í¬ê¸°: {model_size:.1f}GB")

            llm = Llama(
                model_path=model_path,
                n_ctx=4096,
                n_threads=8,
                n_threads_batch=8,
                n_gpu_layers=0,
                verbose=False,
                n_batch=1024,
                use_mlock=False,
                use_mmap=True,
                main_gpu=0,
                numa=False,
                seed=-1,
                flash_attn=True,
            )

            end_time = datetime.now()
            loading_time = (end_time - start_time).total_seconds()
            logger.info(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {loading_time:.2f}ì´ˆ)")
            logger.info("=" * 50)

        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            performance_stats["error_count"] += 1
            raise e


def get_conversation_context(session_id: str, max_turns: int = 5) -> str:
    """ëŒ€í™” ê¸°ë¡ì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±"""
    if session_id not in conversation_history:
        return ""

    history = conversation_history[session_id]
    # ìµœê·¼ max_turns ê°œì˜ ëŒ€í™”ë§Œ í¬í•¨
    recent_history = history[-max_turns * 2:] if len(history) > max_turns * 2 else history

    context = ""
    for entry in recent_history:
        if entry["role"] == "user":
            context += f"ì‚¬ìš©ì: {entry['content']}\n"
        elif entry["role"] == "assistant":
            context += f"AI: {entry['content']}\n"

    return context


def add_to_conversation_history(session_id: str, role: str, content: str):
    """ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€"""
    if session_id not in conversation_history:
        conversation_history[session_id] = []

    conversation_history[session_id].append({
        "role": role,
        "content": content,
        "timestamp": datetime.now().isoformat()
    })

    # ëŒ€í™” ê¸°ë¡ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ì˜¤ë˜ëœ ê²ƒë“¤ ì œê±° (ìµœëŒ€ 20ê°œ ìœ ì§€)
    if len(conversation_history[session_id]) > 20:
        conversation_history[session_id] = conversation_history[session_id][-20:]


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ ì‹¤í–‰
    logger.info("=" * 60)
    logger.info("DH-Kkatuli_v1.0 ì„œë²„ ì‹œì‘")
    logger.info(f"ì‹œì‘ ì‹œê°„: {datetime.now()}")
    logger.info("=" * 60)

    yield  # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰

    # ì¢…ë£Œ ì‹œ ì‹¤í–‰
    logger.info("=" * 60)
    logger.info("DH-Kkatuli_v1.0 ì„œë²„ ì¢…ë£Œ")
    logger.info(f"ì¢…ë£Œ ì‹œê°„: {datetime.now()}")
    log_performance_stats()
    logger.info("=" * 60)


# FastAPI ì•± ìƒì„± ì‹œ lifespan ì„¤ì •
app = FastAPI(lifespan=lifespan)


def create_optimized_prompt(message: str, context: str = "") -> str:
    """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„± - ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨"""
    logger.debug(f"í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘ - ë©”ì‹œì§€ ê¸¸ì´: {len(message)}, ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)}")

    # ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€
    system_msg = """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ì˜ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìˆê³  ì—°ì†ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. 
ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ì´ì–´ê°€ê³ , í•„ìš”ì‹œ ì¶”ê°€ì ì¸ ì •ë³´ë‚˜ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
ê°€ëŠ¥í•œ í•œ ìì„¸í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."""

    if any(keyword in message.lower() for keyword in ['ì½”ë“œ', 'code', 'í”„ë¡œê·¸ë˜ë°', 'íŒŒì´ì¬', 'ìë°”']):
        system_msg += " í”„ë¡œê·¸ë˜ë°ê³¼ ê¸°ìˆ  ë¬¸ì œì— ëŒ€í•´ ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
        logger.debug("ì½”ë”© ê´€ë ¨ í”„ë¡¬í”„íŠ¸ ìƒì„±")
    elif any(keyword in message.lower() for keyword in ['ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ìˆœìœ„', 'ì •ë³´']):
        system_msg += " ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìì„¸í•œ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤. ì¶©ë¶„í•œ ì •ë³´ì™€ ë°°ê²½ ì§€ì‹ì„ í¬í•¨í•˜ì—¬ ì™„ì „í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."
        logger.debug("ì„¤ëª… ê´€ë ¨ í”„ë¡¬í”„íŠ¸ ìƒì„±")

    # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° í¬í•¨
    if context.strip():
        prompt = f"""<|system|>{system_msg}

ì´ì „ ëŒ€í™” ë‚´ìš©:
{context}</s>
<|user|>{message}</s>
<|assistant|>"""
    else:
        prompt = f"""<|system|>{system_msg}</s>
<|user|>{message}</s>
<|assistant|>"""

    return prompt


def is_response_complete(text: str, token_count: int) -> bool:
    """ì‘ë‹µì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸ - ë” ì—„ê²©í•œ ì¡°ê±´ìœ¼ë¡œ ë³€ê²½"""
    logger.debug(f"ì‘ë‹µ ì™„ë£Œ ê²€ì‚¬ - í† í°: {token_count}, í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")

    # ìµœì†Œ í† í° ìˆ˜ í™•ì¸ (ë” ë§ì€ í† í° ìš”êµ¬)
    if token_count < 100:
        logger.debug("ìµœì†Œ í† í° ìˆ˜ ë¯¸ë‹¬")
        return False

    # í…ìŠ¤íŠ¸ ê¸¸ì´ë„ í™•ì¸ (í•œêµ­ì–´ íŠ¹ì„±ìƒ)
    if len(text.strip()) < 200:
        logger.debug("ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¯¸ë‹¬")
        return False

    # ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸
    complete_patterns = [
        r'.+[.!?]\s*$',
        r'.+ì…ë‹ˆë‹¤\.$',
        r'.+ìŠµë‹ˆë‹¤\.$',
        r'.+ë©ë‹ˆë‹¤\.$',
        r'.+ìˆìŠµë‹ˆë‹¤\.$',
        r'.+í•©ë‹ˆë‹¤\.$',
    ]

    # ì¶©ë¶„í•œ ë‚´ìš©ê³¼ ë¬¸ì¥ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    sentences = re.split(r'[.!?]', text.strip())
    if len(sentences) >= 3:
        is_complete = any(re.search(pattern, text.strip()) for pattern in complete_patterns)
        logger.debug(f"ë¬¸ì¥ ì™„ë£Œ ê²€ì‚¬ ê²°ê³¼: {is_complete}")
        return is_complete

    return False


def should_stop_generation(accumulated_text: str, current_token: str, token_count: int) -> bool:
    """ìƒì„±ì„ ì¤‘ë‹¨í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨ - ë” ê´€ëŒ€í•˜ê²Œ ìˆ˜ì •"""
    # ë°˜ë³µ íŒ¨í„´ ê°ì§€ (ë” ì—„ê²©í•˜ê²Œ)
    if token_count > 50:
        words = accumulated_text.split()
        if len(words) >= 15:
            recent_words = words[-7:]
            previous_words = words[-14:-7] if len(words) >= 14 else []
            if recent_words == previous_words and len(set(recent_words)) > 2:
                logger.warning(f"ë°˜ë³µ íŒ¨í„´ ê°ì§€í•˜ì—¬ ìƒì„± ì¤‘ë‹¨: {recent_words}")
                return True

    # ë¶€ì ì ˆí•œ íŒ¨í„´ ê°ì§€
    unwanted_patterns = [
        "ì§ˆë¬¸:", "ë‹µë³€:", "<|user|>", "<|assistant|>", "<|system|>",
        "Human:", "AI:", "\n\nì§ˆë¬¸", "\n\në‹µë³€", "ì‚¬ìš©ì:", "\nì‚¬ìš©ì:"
    ]

    for pattern in unwanted_patterns:
        if pattern in current_token or pattern in accumulated_text[-100:]:
            logger.warning(f"ë¶€ì ì ˆí•œ íŒ¨í„´ ê°ì§€í•˜ì—¬ ìƒì„± ì¤‘ë‹¨: {pattern}")
            return True

    return False


@app.post("/chat")
async def chat_endpoint(request: Request):
    request_start_time = datetime.now()
    request_id = f"REQ_{request_start_time.strftime('%Y%m%d_%H%M%S_%f')}"

    logger.info(f"[{request_id}] ìƒˆë¡œìš´ ì±„íŒ… ìš”ì²­ ì‹œì‘")
    performance_stats["total_requests"] += 1

    try:
        # í´ë¼ì´ì–¸íŠ¸ IP ë¡œê¹…
        client_ip = request.client.host
        logger.info(f"[{request_id}] í´ë¼ì´ì–¸íŠ¸ IP: {client_ip}")

        # ëª¨ë¸ ì´ˆê¸°í™” í™•ì¸
        if llm is None:
            logger.info(f"[{request_id}] ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ, ì´ˆê¸°í™” ì‹œì‘")
            optimize_system()
            initialize_model()

        data = await request.json()
        user_message = data.get("message", "").strip()
        session_id = data.get("session_id", "default")  # ì„¸ì…˜ ID ì¶”ê°€
        continue_conversation = data.get("continue", False)  # ëŒ€í™” ì´ì–´ê°€ê¸° í”Œë˜ê·¸

        if not user_message:
            logger.warning(f"[{request_id}] ë¹ˆ ë©”ì‹œì§€ ìš”ì²­")
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        logger.info(f"[{request_id}] ì‚¬ìš©ì ë©”ì‹œì§€: {user_message[:100]}{'...' if len(user_message) > 100 else ''}")
        logger.info(f"[{request_id}] ì„¸ì…˜ ID: {session_id}, ëŒ€í™” ì´ì–´ê°€ê¸°: {continue_conversation}")

        # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        add_to_conversation_history(session_id, "user", user_message)

        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = ""
        if continue_conversation or session_id in conversation_history:
            context = get_conversation_context(session_id)
            logger.info(f"[{request_id}] ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ë¬¸ì")

        # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        prompt = create_optimized_prompt(user_message, context)
        logger.debug(f"[{request_id}] ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}")

        def generate():
            generation_start_time = datetime.now()
            token_count = 0
            accumulated_text = ""

            try:
                logger.info(f"[{request_id}] í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘")

                response = llm(
                    prompt,
                    max_tokens=3072,
                    temperature=0.8,
                    top_p=0.95,
                    top_k=50,
                    repeat_penalty=1.1,
                    frequency_penalty=0.1,
                    presence_penalty=0.05,
                    echo=False,
                    stream=True,
                    stop=["</s>", "<|user|>", "<|system|>"]
                )

                for chunk in response:
                    if 'choices' in chunk and len(chunk['choices']) > 0:
                        text = chunk['choices'][0].get('text', '')
                        if text:
                            accumulated_text += text
                            token_count += 1

                            # ì¤‘ë‹¨ ì¡°ê±´ ê²€ì‚¬
                            if should_stop_generation(accumulated_text, text, token_count):
                                logger.warning(f"[{request_id}] ë¶€ì ì ˆí•œ íŒ¨í„´ ê°ì§€ë¡œ ìƒì„± ì¤‘ë‹¨")
                                break

                            # ì£¼ê¸°ì  ë¡œê¹…
                            if token_count % 50 == 0:
                                logger.info(f"[{request_id}] ì§„í–‰ ìƒí™© - í† í°: {token_count}, ê¸¸ì´: {len(accumulated_text)}")

                            yield f"data: {json.dumps({'text': text}, ensure_ascii=False)}\n\n"

                            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì™„ë£Œ ì²´í¬
                            if text in '.!?' and is_response_complete(accumulated_text, token_count):
                                logger.info(f"[{request_id}] ë¬¸ì¥ ì™„ë£Œ ê°ì§€ë¡œ ìƒì„± ì¢…ë£Œ")
                                break

                            # ìµœëŒ€ í† í° ìˆ˜ ì œí•œ
                            if token_count > 1500:
                                logger.info(f"[{request_id}] ìµœëŒ€ í† í° ìˆ˜ ë„ë‹¬ë¡œ ìƒì„± ì¤‘ë‹¨")
                                break

                # ëŒ€í™” ê¸°ë¡ì— AI ì‘ë‹µ ì¶”ê°€
                if accumulated_text.strip():
                    add_to_conversation_history(session_id, "assistant", accumulated_text.strip())

                generation_end_time = datetime.now()
                generation_time = (generation_end_time - generation_start_time).total_seconds()

                # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
                performance_stats["total_tokens_generated"] += token_count
                performance_stats["total_processing_time"] += generation_time

                logger.info(
                    f"[{request_id}] ìƒì„± ì™„ë£Œ - í† í°: {token_count}, ì‹œê°„: {generation_time:.2f}ì´ˆ, ì†ë„: {token_count / max(generation_time, 0.001):.2f}í† í°/ì´ˆ")
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"[{request_id}] ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                logger.error(f"[{request_id}] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                performance_stats["error_count"] += 1
                error_msg = json.dumps({'error': f'ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}, ensure_ascii=False)
                yield f"data: {error_msg}\n\n"

        return StreamingResponse(generate(), media_type="text/event-stream")

    except Exception as e:
        logger.error(f"[{request_id}] ì±— ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}")
        logger.error(f"[{request_id}] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        performance_stats["error_count"] += 1
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        request_end_time = datetime.now()
        total_request_time = (request_end_time - request_start_time).total_seconds()
        logger.info(f"[{request_id}] ìš”ì²­ ì™„ë£Œ - ì´ ì†Œìš” ì‹œê°„: {total_request_time:.2f}ì´ˆ")

        # ë§¤ 10ë²ˆì§¸ ìš”ì²­ë§ˆë‹¤ ì„±ëŠ¥ í†µê³„ ì¶œë ¥
        if performance_stats["total_requests"] % 10 == 0:
            log_performance_stats()


@app.get("/history/{session_id}")
async def get_conversation_history(session_id: str):
    """íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
    logger.info(f"ëŒ€í™” ê¸°ë¡ ì¡°íšŒ ìš”ì²­ - ì„¸ì…˜ ID: {session_id}")
    if session_id in conversation_history:
        return {"session_id": session_id, "history": conversation_history[session_id]}
    return {"session_id": session_id, "history": []}


@app.delete("/history/{session_id}")
async def clear_conversation_history(session_id: str):
    """íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
    logger.info(f"ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ìš”ì²­ - ì„¸ì…˜ ID: {session_id}")
    if session_id in conversation_history:
        del conversation_history[session_id]
        return {"message": f"ì„¸ì…˜ {session_id}ì˜ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}
    return {"message": f"ì„¸ì…˜ {session_id}ì˜ ëŒ€í™” ê¸°ë¡ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}


@app.get("/")
async def get_chat_interface():
    logger.info("ì›¹ ì¸í„°í˜ì´ìŠ¤ ìš”ì²­")
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>DH-Kkatuli_v1.0 (ëŒ€í™” ì—°ì† ê¸°ëŠ¥ ì¶”ê°€)</title>
        <meta charset="UTF-8">
        <style>
            #chat-container { 
                max-width: 900px; 
                margin: 20px auto; 
                padding: 20px;
                font-family: 'Malgun Gothic', Arial, sans-serif;
            }
            #messages { 
                margin-bottom: 20px;
                padding: 15px;
                height: 500px;
                overflow-y: auto;
                border: 1px solid #ddd;
                border-radius: 8px;
                background-color: #fafafa;
            }
            .message { 
                margin: 15px 0;
                padding: 12px 15px;
                border-radius: 8px;
                word-wrap: break-word;
                line-height: 1.5;
            }
            .user { 
                background-color: #e3f2fd;
                margin-left: 15%;
                text-align: right;
                border-left: 4px solid #2196f3;
            }
            .assistant { 
                background-color: #f8f9fa;
                margin-right: 15%;
                border-left: 4px solid #28a745;
            }
            #input-container {
                display: flex;
                gap: 10px;
                margin-top: 15px;
            }
            #control-container {
                display: flex;
                gap: 10px;
                margin-bottom: 10px;
            }
            #user-input {
                flex-grow: 1;
                padding: 12px;
                border: 2px solid #ddd;
                border-radius: 6px;
                font-size: 14px;
                transition: border-color 0.3s;
            }
            #user-input:focus {
                border-color: #2196f3;
                outline: none;
            }
            button {
                padding: 12px 24px;
                background-color: #2196f3;
                color: white;
                border: none;
                border-radius: 6px;
                cursor: pointer;
                font-size: 14px;
                transition: background-color 0.3s;
            }
            button:hover {
                background-color: #1976d2;
            }
            button:disabled {
                background-color: #ccc;
                cursor: not-allowed;
            }
            #stop-btn {
                background-color: #f44336;
                display: none;
            }
            #stop-btn:hover {
                background-color: #d32f2f;
            }
            #continue-btn {
                background-color: #ff9800;
            }
            #continue-btn:hover {
                background-color: #f57c00;
            }
            #clear-btn {
                background-color: #9e9e9e;
            }
            #clear-btn:hover {
                background-color: #757575;
            }
            .loading {
                color: #666;
                font-style: italic;
                animation: pulse 1.5s infinite;
            }
            @keyframes pulse {
                0% { opacity: 1; }
                50% { opacity: 0.5; }
                100% { opacity: 1; }
            }
            .stopped {
                color: #666;
                font-style: italic;
                border-left: 4px solid #f44336;
            }
            .error {
                background-color: #ffebee;
                border-left: 4px solid #f44336;
                color: #c62828;
            }
            .stats {
                font-size: 12px;
                color: #666;
                margin-top: 10px;
                text-align: center;
            }
            .session-info {
                font-size: 12px;
                color: #666;
                margin-bottom: 10px;
                padding: 8px;
                background-color: #f5f5f5;
                border-radius: 4px;
            }
        </style>
    </head>
    <body>
        <div id="chat-container">
            <h1>ğŸ¤– DH-Kkatuli_v1.0 (ëŒ€í™” ì—°ì† ê¸°ëŠ¥ ì¶”ê°€)</h1>
            <div class="stats">
                <span>ëª¨ë¸: EEVE-Korean-Instruct-10.8B | ëŒ€í™” ê¸°ë¡ ìœ ì§€ | ì—°ì† ë‹µë³€ ê¸°ëŠ¥</span>
            </div>
            <div class="session-info">
                ì„¸ì…˜ ID: <span id="session-id">default</span> | 
                ëŒ€í™” ìˆ˜: <span id="message-count">0</span>
            </div>
            <div id="control-container">
                <button id="continue-btn" onclick="continueConversation()">ğŸ”„ ëŒ€í™” ì´ì–´ê°€ê¸°</button>
                <button id="clear-btn" onclick="clearHistory()">ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸°</button>
            </div>
            <div id="messages"></div>
            <div id="input-container">
                <input type="text" id="user-input" placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (Enterë¡œ ì „ì†¡)">
                <button id="send-btn" onclick="sendMessage()">ğŸ’¬ ì „ì†¡</button>
                <button id="stop-btn" onclick="stopGeneration()">â¹ï¸ ì¤‘ë‹¨</button>
            </div>
        </div>

        <script>
            let currentController = null;
            let isGenerating = false;
            let messageCount = 0;
            let sessionId = 'default';

            function addMessage(role, content, isLoading = false) {
                const messagesDiv = document.getElementById('messages');
                const messageDiv = document.createElement('div');
                messageDiv.className = `message ${role}${isLoading ? ' loading' : ''}`;
                messageDiv.textContent = content;
                messagesDiv.appendChild(messageDiv);
                messagesDiv.scrollTop = messagesDiv.scrollHeight;
                messageCount++;
                updateMessageCount();
                return messageDiv;
            }

            function updateMessageCount() {
                document.getElementById('message-count').textContent = Math.floor(messageCount / 2);
            }

            function updateButtonStates(generating) {
                const sendBtn = document.getElementById('send-btn');
                const stopBtn = document.getElementById('stop-btn');
                const userInput = document.getElementById('user-input');
                const continueBtn = document.getElementById('continue-btn');
                const clearBtn = document.getElementById('clear-btn');

                isGenerating = generating;
                sendBtn.disabled = generating;
                userInput.disabled = generating;
                continueBtn.disabled = generating;
                clearBtn.disabled = generating;

                if (generating) {
                    stopBtn.style.display = 'inline-block';
                    sendBtn.style.display = 'none';
                } else {
                    stopBtn.style.display = 'none';
                    sendBtn.style.display = 'inline-block';
                }
            }

            function stopGeneration() {
                if (currentController) {
                    currentController.abort();
                    currentController = null;
                }
                updateButtonStates(false);

                const messages = document.querySelectorAll('.message.assistant');
                if (messages.length > 0) {
                    const lastMessage = messages[messages.length - 1];
                    if (!lastMessage.textContent.includes('[ì¤‘ë‹¨ë¨]')) {
                        lastMessage.textContent += ' [ì¤‘ë‹¨ë¨]';
                        lastMessage.classList.add('stopped');
                    }
                }
            }

            async function sendMessage(continueMode = false) {
                const input = document.getElementById('user-input');
                let message = input.value.trim();

                if (continueMode) {
                    // ëŒ€í™” ì´ì–´ê°€ê¸° ëª¨ë“œì—ì„œëŠ” ê¸°ë³¸ ë©”ì‹œì§€ ì‚¬ìš©
                    message = message || "ìœ„ì˜ ë‹µë³€ì— ì´ì–´ì„œ ë” ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.";
                }

                if (!message || isGenerating) return;

                input.value = '';
                updateButtonStates(true);

                if (!continueMode) {
                    addMessage('user', message);
                }

                const loadingMsg = addMessage('assistant', 'ğŸ¤” ìƒê°í•˜ëŠ” ì¤‘...', true);

                currentController = new AbortController();
                const startTime = Date.now();

                try {
                    const requestBody = {
                        message: message,
                        session_id: sessionId,
                        continue: continueMode
                    };

                    const response = await fetch('/chat', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify(requestBody),
                        signal: currentController.signal
                    });

                    if (!response.ok) {
                        throw new Error(`ì„œë²„ ì˜¤ë¥˜: ${response.status}`);
                    }

                    const reader = response.body.getReader();
                    const decoder = new TextDecoder();
                    let assistantResponse = '';

                    loadingMsg.remove();
                    const responseDiv = addMessage('assistant', '');

                    try {
                        while (true) {
                            const { value, done } = await reader.read();
                            if (done) break;

                            const chunk = decoder.decode(value);
                            const lines = chunk.split('\\n');

                            for (const line of lines) {
                                if (line.startsWith('data: ')) {
                                    const data = line.slice(6);
                                    if (data === '[DONE]') {
                                        const duration = ((Date.now() - startTime) / 1000).toFixed(1);
                                        console.log(`ì‘ë‹µ ì™„ë£Œ (${duration}ì´ˆ)`);
                                        break;
                                    }

                                    try {
                                        const parsed = JSON.parse(data);
                                        if (parsed.text) {
                                            assistantResponse += parsed.text;
                                            responseDiv.textContent = assistantResponse;
                                            responseDiv.scrollIntoView({ behavior: 'smooth', block: 'end' });
                                        } else if (parsed.error) {
                                            responseDiv.textContent = `âŒ ${parsed.error}`;
                                            responseDiv.className += ' error';
                                            break;
                                        }
                                    } catch (e) {
                                        console.error('JSON íŒŒì‹± ì˜¤ë¥˜:', e);
                                    }
                                }
                            }
                        }
                    } catch (readError) {
                        if (readError.name === 'AbortError') {
                            console.log('ìŠ¤íŠ¸ë¦¼ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨');
                        } else {
                            throw readError;
                        }
                    }

                    if (!assistantResponse.trim() && !responseDiv.classList.contains('stopped')) {
                        responseDiv.textContent = 'âŒ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.';
                        responseDiv.className += ' error';
                    }

                } catch (error) {
                    if (error.name === 'AbortError') {
                        console.log('ìš”ì²­ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨');
                        if (loadingMsg.parentNode) {
                            loadingMsg.textContent = 'â¹ï¸ ìš”ì²­ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.';
                            loadingMsg.className = 'message assistant stopped';
                        }
                    } else {
                        console.error('ì˜¤ë¥˜:', error);
                        if (loadingMsg.parentNode) {
                            loadingMsg.textContent = `âŒ ì˜¤ë¥˜: ${error.message}`;
                            loadingMsg.className = 'message assistant error';
                        }
                    }
                } finally {
                    updateButtonStates(false);
                    currentController = null;
                }
            }

            function continueConversation() {
                sendMessage(true);
            }

            async function clearHistory() {
                if (confirm('ëŒ€í™” ê¸°ë¡ì„ ëª¨ë‘ ì§€ìš°ì‹œê² ìŠµë‹ˆê¹Œ?')) {
                    try {
                        await fetch(`/history/${sessionId}`, {
                            method: 'DELETE'
                        });
                        document.getElementById('messages').innerHTML = '';
                        messageCount = 0;
                        updateMessageCount();
                        console.log('ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.');
                    } catch (error) {
                        console.error('ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ì˜¤ë¥˜:', error);
                    }
                }
            }

            // ì—”í„° í‚¤ ì´ë²¤íŠ¸
            document.getElementById('user-input').addEventListener('keypress', function(e) {
                if (e.key === 'Enter' && !e.shiftKey && !isGenerating) {
                    e.preventDefault();
                    sendMessage();
                }
            });

            // í˜ì´ì§€ ì–¸ë¡œë“œ ì‹œ ì •ë¦¬
            window.addEventListener('beforeunload', function() {
                if (currentController) {
                    currentController.abort();
                }
            });

            // ì´ˆê¸° í¬ì»¤ìŠ¤
            document.getElementById('user-input').focus();
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content, status_code=200)


if __name__ == "__main__":
    import uvicorn

    logger.info("ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì¤‘...")
    uvicorn.run(app, host="127.0.0.1", port=8000)